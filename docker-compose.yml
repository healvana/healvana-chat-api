  services:
    ollama:
      build:
        context: ./ollama
        # For production, consider pre-building and tagging images:
        # image: your-registry/ollama-healvana:latest
      volumes:
        - ollama_data:/root/.ollama # Use a named volume for clarity and management
      ports:
        - "11434:11434"
      networks:
        - app-network
      environment:
        # These should be defined in your .env file or passed by the deployment system
        - OLLAMA_MODEL=${OLLAMA_MODEL}
        - EMBEDDINGS_MODEL=${EMBEDDINGS_MODEL}
        # Consider Ollama-specific configurations for production:
        # - OLLAMA_HOST=0.0.0.0 # Already default behavior for ollama serve
        # - OLLAMA_NUM_PARALLEL= # Adjust based on your hardware
        # - OLLAMA_MAX_LOADED_MODELS= # If you use many models
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:11434/"] # Basic check if Ollama API is up
        interval: 30s
        timeout: 10s
        retries: 10
        start_period: 60s # Give Ollama time to start and potentially pull initial models
      restart: unless-stopped
      deploy: # Resource constraints (tune these based on your needs and hardware)
        resources:
          limits:
            cpus: '2.0' # Example: limit to 2 CPU cores
            memory: 8G  # Example: limit to 8GB RAM
          reservations:
            cpus: '0.5'
            memory: 2G

    langchain:
      build:
        context: ./langchain
        # For production, consider pre-building and tagging images:
        # image: your-registry/langchain-healvana-api:latest
      volumes:
        # In production, code should ideally be baked into the image.
        # This volume mount is more for development.
        # If you keep it for prod-like local testing, ensure paths are correct.
        - ./langchain:/app
      networks:
        - app-network
      depends_on:
        ollama:
          condition: service_healthy # Wait for Ollama to be healthy
      ports:
        - "8000:8000"
      environment:
        # These should be defined in your .env file or passed by the deployment system
        - OLLAMA_MODEL=${OLLAMA_MODEL}
        - EMBEDDINGS_MODEL=${EMBEDDINGS_MODEL} # Ensure this is passed
        - OLLAMA_HOST=ollama
        - OLLAMA_PORT=11434
        - OLLAMA_TEMPERATURE=${OLLAMA_TEMPERATURE:-0.2} # Default if not set in .env
        - DEFAULT_LOCALE=${DEFAULT_LOCALE:-en-US}       # Default if not set in .env
        - LOG_LEVEL=${LOG_LEVEL:-INFO}                   # Default if not set in .env
        # For production, consider using a persistent session store if sessions need to survive restarts
        # - REDIS_HOST=redis # Example if using Redis
        # - REDIS_PORT=6379
      command: poetry run python serve.py
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8000/health"] # Points to your FastAPI health endpoint
        interval: 30s
        timeout: 10s
        retries: 3
        start_period: 30s # Give the FastAPI app time to start
      restart: unless-stopped
      deploy:
        resources:
          limits:
            cpus: '1.0'
            memory: 2G
          reservations:
            cpus: '0.25'
            memory: 512M

    web:
      build:
        context: ./web
        # For production, consider pre-building and tagging images:
        # image: your-registry/web-healvana-frontend:latest
      ports:
        - "8087:80" # Assuming your Svelte app's container (e.g., Nginx) serves on port 80
      depends_on:
        langchain:
          condition: service_healthy # Wait for the backend to be healthy
      networks:
        - app-network
      environment:
        # These should be configured based on how your frontend expects them
        # VITE_API_HOST is common for Vite apps, ensure it's correct for your frontend
        - VITE_API_HOST=langchain
        - VITE_API_PORT=8000 # If your frontend needs to know the backend port explicitly
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:80/"] # Basic check if web server is responding
        interval: 30s
        timeout: 10s
        retries: 3
        start_period: 10s
      restart: unless-stopped
      deploy:
        resources:
          limits:
            cpus: '0.5'
            memory: 512M
          reservations:
            cpus: '0.1'
            memory: 128M

  networks:
    app-network:
      driver: bridge # Default, good for single-host deployments

  volumes:
    ollama_data: # Explicitly define the named volume for Ollama
      # For production, consider external storage or backup strategies for this volume
